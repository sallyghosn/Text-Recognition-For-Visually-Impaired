{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CRNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jYf4TrLWJTW"
      },
      "source": [
        "CHAR_VECTOR = \"abcdefghijklmnoqqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789-_$\"\n",
        "\n",
        "letters = [letter for letter in CHAR_VECTOR]\n",
        "\n",
        "num_classes = len(letters) + 1\n",
        "\n",
        "img_w, img_h = 128, 64\n",
        "\n",
        "# Network parameters\n",
        "batch_size = 128\n",
        "val_batch_size = 16\n",
        "\n",
        "downsample_factor = 4\n",
        "max_text_len = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXwF9KkbWTzM"
      },
      "source": [
        "# char_list:   'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'\n",
        "# total number of our output classes: len(char_list)\n",
        "char_list = \"abcdefghijklmnoqqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789-_$\"\n",
        " \n",
        "def encode_to_labels(txt):\n",
        "    # encoding each output word into digits\n",
        "    dig_lst = []\n",
        "    for index, char in enumerate(txt):\n",
        "        try:\n",
        "            dig_lst.append(char_list.index(char))\n",
        "        except:\n",
        "            print(char)\n",
        "        \n",
        "    return dig_lst\n",
        "import os\n",
        "import fnmatch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import string\n",
        "import time\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from keras.layers import Dense, LSTM, Reshape, BatchNormalization, Input, Conv2D, MaxPool2D, Lambda, Bidirectional\n",
        "from keras.models import Model\n",
        "from keras.activations import relu, sigmoid, softmax\n",
        "import keras.backend as K\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_PWVUiyd8cS",
        "outputId": "17b115e4-fd80-4455-f2d8-6438589c8f5b"
      },
      "source": [
        "train_dir = \"/content/drive/MyDrive/minidata1\"\n",
        "val_dir =\"/content/drive/MyDrive/minidat\"\n",
        " \n",
        " \n",
        "# lists for training dataset\n",
        "training_img = []\n",
        "training_txt = []\n",
        "train_input_length = []\n",
        "train_label_length = []\n",
        "orig_txt = []\n",
        " \n",
        "#lists for validation dataset\n",
        "valid_img = []\n",
        "valid_txt = []\n",
        "valid_input_length = []\n",
        "valid_label_length = []\n",
        "valid_orig_txt = []\n",
        " \n",
        "max_label_len = 0\n",
        " \n",
        "i =1 \n",
        "flag = 0\n",
        " \n",
        "for root, dirnames, filenames in os.walk(train_dir):\n",
        " \n",
        "    for f_name in fnmatch.filter(filenames, '*.jpg'):\n",
        "        # read input image and convert into gray scale image\n",
        "        img = cv2.cvtColor(cv2.imread(os.path.join(root, f_name)), cv2.COLOR_BGR2GRAY)   \n",
        " \n",
        "        # convert each image of shape (32, 128, 1)\n",
        "        w, h = img.shape\n",
        "        if h > 128 or w > 32:\n",
        "            continue\n",
        "        if w < 32:\n",
        "            add_zeros = np.ones((32-w, h))*255\n",
        "            img = np.concatenate((img, add_zeros))\n",
        " \n",
        "        if h < 128:\n",
        "            add_zeros = np.ones((32, 128-h))*255\n",
        "            img = np.concatenate((img, add_zeros), axis=1)\n",
        "        img = np.expand_dims(img , axis = 2)\n",
        "        \n",
        "        # Normalize each image\n",
        "        img = img/255.\n",
        "        \n",
        "        # get the text from the image\n",
        "        txt = f_name.split('_')[1]\n",
        "        \n",
        "        # compute maximum length of the text\n",
        "        if len(txt) > max_label_len:\n",
        "            max_label_len = len(txt)\n",
        "            \n",
        "           \n",
        "      \n",
        "            orig_txt.append(txt)   \n",
        "            train_label_length.append(len(txt))\n",
        "            train_input_length.append(31)\n",
        "            training_img.append(img)\n",
        "            training_txt.append(encode_to_labels(txt)) \n",
        "              # break the loop if total data is 150000\n",
        "        if i == 5000:\n",
        "            flag = 1\n",
        "            break\n",
        "        i+=1\n",
        "    if flag == 1:\n",
        "        break\n",
        " \n",
        "j =1 \n",
        "flag1 = 0\n",
        "max_label_len1=0\n",
        "for root, dirnames, filenames in os.walk(val_dir):\n",
        " \n",
        "    for f_name in fnmatch.filter(filenames, '*.jpg'):\n",
        "        # read input image and convert into gray scale image\n",
        "        img = cv2.cvtColor(cv2.imread(os.path.join(root, f_name)), cv2.COLOR_BGR2GRAY)   \n",
        " \n",
        "        # convert each image of shape (32, 128, 1)\n",
        "        w, h = img.shape\n",
        "        if h > 128 or w > 32:\n",
        "            continue\n",
        "        if w < 32:\n",
        "            add_zeros = np.ones((32-w, h))*255\n",
        "            img = np.concatenate((img, add_zeros))\n",
        " \n",
        "        if h < 128:\n",
        "            add_zeros = np.ones((32, 128-h))*255\n",
        "            img = np.concatenate((img, add_zeros), axis=1)\n",
        "        img = np.expand_dims(img , axis = 2)\n",
        "        \n",
        "        # Normalize each image\n",
        "        img = img/255.\n",
        "        \n",
        "        # get the text from the image\n",
        "        txt = f_name.split('_')[1]\n",
        "        \n",
        "        # compute maximum length of the text\n",
        "        if len(txt) > max_label_len1:\n",
        "            max_label_len1 = len(txt)\n",
        "       \n",
        "        valid_orig_txt.append(txt)   \n",
        "        valid_label_length.append(len(txt))\n",
        "        valid_input_length.append(31)\n",
        "        valid_img.append(img)\n",
        "        valid_txt.append(encode_to_labels(txt))\n",
        "       \n",
        "        if j == 3000:\n",
        "            flag1 = 1\n",
        "            break\n",
        "        j+=1\n",
        "    if flag1 == 1:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n",
            "p\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pg6goHj7gOKl",
        "outputId": "a0fc0728-2a17-4db0-ce87-cc80151db395"
      },
      "source": [
        "train_padded_txt = pad_sequences(training_txt, maxlen=max_label_len, padding='post', value = len(char_list))\n",
        "valid_padded_txt = pad_sequences(valid_txt, maxlen=max_label_len, padding='post', value = len(char_list))\n",
        "print(train_padded_txt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[45 17 14 20 13  2  4 65 65 65 65 65 65 65 65 65 65]\n",
            " [28  0 13 21  0 18  8 13  6 65 65 65 65 65 65 65 65]\n",
            " [31  0 13 19  0 18  8 25  4  3 65 65 65 65 65 65 65]\n",
            " [ 2 14 17 17  4 18 14 13  3  4 13 19 65 65 65 65 65]\n",
            " [44  2  7 14 14 11  2  7  8 11  3 17  4 13 65 65 65]\n",
            " [28 14 13  6 17  4  6  0 19  8 14 13  0 11  8 18 19]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oM0yfIujWgJ8",
        "outputId": "97a5f0f4-e31a-4077-f6fd-f58259b3e494"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y25Dck7bWo9M",
        "outputId": "aa18f8d2-260b-4dc4-b429-0373cb6de6ea"
      },
      "source": [
        "# input with shape of height=32 and width=128 \n",
        "inputs = Input(shape=(32,128,1))\n",
        " \n",
        "# convolution layer with kernel size (3,3)\n",
        "conv_1 = Conv2D(64, (3,3), activation = 'relu', padding='same')(inputs)\n",
        "# poolig layer with kernel size (2,2)\n",
        "pool_1 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_1)\n",
        " \n",
        "conv_2 = Conv2D(128, (3,3), activation = 'relu', padding='same')(pool_1)\n",
        "pool_2 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_2)\n",
        " \n",
        "conv_3 = Conv2D(256, (3,3), activation = 'relu', padding='same')(pool_2)\n",
        " \n",
        "conv_4 = Conv2D(256, (3,3), activation = 'relu', padding='same')(conv_3)\n",
        "# poolig layer with kernel size (2,1)\n",
        "pool_4 = MaxPool2D(pool_size=(2, 1))(conv_4)\n",
        " \n",
        "conv_5 = Conv2D(512, (3,3), activation = 'relu', padding='same')(pool_4)\n",
        "# Batch normalization layer\n",
        "batch_norm_5 = BatchNormalization()(conv_5)\n",
        " \n",
        "conv_6 = Conv2D(512, (3,3), activation = 'relu', padding='same')(batch_norm_5)\n",
        "batch_norm_6 = BatchNormalization()(conv_6)\n",
        "pool_6 = MaxPool2D(pool_size=(2, 1))(batch_norm_6)\n",
        " \n",
        "conv_7 = Conv2D(512, (2,2), activation = 'relu')(pool_6)\n",
        " \n",
        "squeezed = Lambda(lambda x: K.squeeze(x, 1))(conv_7)\n",
        " \n",
        "# bidirectional LSTM layers with units=128\n",
        "blstm_1 = Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2))(squeezed)\n",
        "blstm_2 = Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2))(blstm_1)\n",
        " \n",
        "outputs = Dense(len(char_list)+1, activation = 'softmax')(blstm_2)\n",
        "\n",
        "# model to be used at test time\n",
        "act_model = Model(inputs, outputs)\n",
        "act_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 32, 128, 1)]      0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 32, 128, 64)       640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 16, 64, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 16, 64, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 8, 32, 128)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 8, 32, 256)        295168    \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 8, 32, 256)        590080    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 4, 32, 256)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 4, 32, 512)        1180160   \n",
            "_________________________________________________________________\n",
            "batch_normalization_36 (Batc (None, 4, 32, 512)        2048      \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 4, 32, 512)        2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_37 (Batc (None, 4, 32, 512)        2048      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 2, 32, 512)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 1, 31, 512)        1049088   \n",
            "_________________________________________________________________\n",
            "lambda_8 (Lambda)            (None, 31, 512)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 31, 256)           656384    \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 31, 256)           394240    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 31, 66)            16962     \n",
            "=================================================================\n",
            "Total params: 6,620,482\n",
            "Trainable params: 6,618,434\n",
            "Non-trainable params: 2,048\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6jGH4CbWvrP"
      },
      "source": [
        "\n",
        "labels = Input(name='the_labels', shape=[max_label_len], dtype='float32')\n",
        "input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
        "label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
        " \n",
        " \n",
        "def ctc_lambda_func(args):\n",
        "    y_pred, labels, input_length, label_length = args\n",
        " \n",
        "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
        " \n",
        " \n",
        "loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([outputs, labels, input_length, label_length])\n",
        "\n",
        "model = Model(inputs=[inputs, labels, input_length, label_length], outputs=loss_out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9QqSD_-ozXN"
      },
      "source": [
        "model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer = 'adam')\n",
        "filepath=\"CRNN_model.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9EqzfR_o4_m"
      },
      "source": [
        "training_img = np.array(training_img)\n",
        "train_input_length = np.array(train_input_length)\n",
        "train_label_length = np.array(train_label_length)\n",
        "\n",
        "valid_img = np.array(valid_img)\n",
        "valid_input_length = np.array(valid_input_length)\n",
        "valid_label_length = np.array(valid_label_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZvB_EBLo61f",
        "outputId": "1b120bbb-8472-43f5-b456-fb16a6236bb7"
      },
      "source": [
        "batch_size = 256\n",
        "epochs = 128\n",
        "step_per_epoch= 16\n",
        "hist = model.fit(x=[training_img, train_padded_txt, train_input_length, train_label_length], y=np.zeros(len(training_img)), batch_size=batch_size, epochs = epochs, validation_data = ([valid_img, valid_padded_txt, valid_input_length, valid_label_length], [np.zeros(len(valid_img))]), verbose = 1, callbacks = callbacks_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 55.1018 - val_loss: 46.6539\n",
            "\n",
            "Epoch 00001: val_loss improved from 105.13238 to 46.65385, saving model to CRNN_model.hdf5\n",
            "Epoch 2/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 46.9175 - val_loss: 32.2533\n",
            "\n",
            "Epoch 00002: val_loss improved from 46.65385 to 32.25334, saving model to CRNN_model.hdf5\n",
            "Epoch 3/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 46.1675 - val_loss: 30.9495\n",
            "\n",
            "Epoch 00003: val_loss improved from 32.25334 to 30.94949, saving model to CRNN_model.hdf5\n",
            "Epoch 4/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 43.0742 - val_loss: 30.5731\n",
            "\n",
            "Epoch 00004: val_loss improved from 30.94949 to 30.57312, saving model to CRNN_model.hdf5\n",
            "Epoch 5/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 40.2811 - val_loss: 30.5864\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 30.57312\n",
            "Epoch 6/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 39.1383 - val_loss: 30.7727\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 30.57312\n",
            "Epoch 7/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 38.2520 - val_loss: 30.7100\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 30.57312\n",
            "Epoch 8/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 37.0815 - val_loss: 30.5770\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 30.57312\n",
            "Epoch 9/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 35.7523 - val_loss: 30.6262\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 30.57312\n",
            "Epoch 10/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 35.0665 - val_loss: 30.8061\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 30.57312\n",
            "Epoch 11/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 34.6606 - val_loss: 31.0661\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 30.57312\n",
            "Epoch 12/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 33.8795 - val_loss: 31.3800\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 30.57312\n",
            "Epoch 13/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 33.2593 - val_loss: 31.7612\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 30.57312\n",
            "Epoch 14/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 32.7894 - val_loss: 32.1363\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 30.57312\n",
            "Epoch 15/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 32.4335 - val_loss: 32.4481\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 30.57312\n",
            "Epoch 16/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 31.8738 - val_loss: 32.6796\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 30.57312\n",
            "Epoch 17/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 31.2939 - val_loss: 32.9098\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 30.57312\n",
            "Epoch 18/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 30.7053 - val_loss: 33.1506\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 30.57312\n",
            "Epoch 19/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 30.2827 - val_loss: 33.3939\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 30.57312\n",
            "Epoch 20/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 30.0203 - val_loss: 33.6526\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 30.57312\n",
            "Epoch 21/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 29.5852 - val_loss: 33.9121\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 30.57312\n",
            "Epoch 22/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 29.0888 - val_loss: 34.1543\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 30.57312\n",
            "Epoch 23/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 28.5165 - val_loss: 34.2566\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 30.57312\n",
            "Epoch 24/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 28.0882 - val_loss: 34.4551\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 30.57312\n",
            "Epoch 25/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 27.6960 - val_loss: 34.6034\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 30.57312\n",
            "Epoch 26/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 27.2462 - val_loss: 34.8193\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 30.57312\n",
            "Epoch 27/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 26.8908 - val_loss: 35.0470\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 30.57312\n",
            "Epoch 28/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 26.2295 - val_loss: 35.2310\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 30.57312\n",
            "Epoch 29/128\n",
            "1/1 [==============================] - 59s 59s/step - loss: 25.8452 - val_loss: 35.4721\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 30.57312\n",
            "Epoch 30/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 25.5049 - val_loss: 35.7743\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 30.57312\n",
            "Epoch 31/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 24.9187 - val_loss: 36.0912\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 30.57312\n",
            "Epoch 32/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 24.5912 - val_loss: 36.3214\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 30.57312\n",
            "Epoch 33/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 24.1257 - val_loss: 36.4236\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 30.57312\n",
            "Epoch 34/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 23.5711 - val_loss: 36.5879\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 30.57312\n",
            "Epoch 35/128\n",
            "1/1 [==============================] - 59s 59s/step - loss: 23.1060 - val_loss: 36.9309\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 30.57312\n",
            "Epoch 36/128\n",
            "1/1 [==============================] - 59s 59s/step - loss: 22.8214 - val_loss: 37.2637\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 30.57312\n",
            "Epoch 37/128\n",
            "1/1 [==============================] - 59s 59s/step - loss: 22.2976 - val_loss: 37.4577\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 30.57312\n",
            "Epoch 38/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 22.2576 - val_loss: 37.3636\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 30.57312\n",
            "Epoch 39/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 21.5663 - val_loss: 37.4966\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 30.57312\n",
            "Epoch 40/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 21.4013 - val_loss: 37.7070\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 30.57312\n",
            "Epoch 41/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 20.6941 - val_loss: 38.0034\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 30.57312\n",
            "Epoch 42/128\n",
            "1/1 [==============================] - 59s 59s/step - loss: 20.5118 - val_loss: 38.3136\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 30.57312\n",
            "Epoch 43/128\n",
            "1/1 [==============================] - 59s 59s/step - loss: 20.0185 - val_loss: 38.5489\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 30.57312\n",
            "Epoch 44/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 19.7702 - val_loss: 38.8835\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 30.57312\n",
            "Epoch 45/128\n",
            "1/1 [==============================] - 59s 59s/step - loss: 19.2429 - val_loss: 39.0615\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 30.57312\n",
            "Epoch 46/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 19.1200 - val_loss: 39.0491\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 30.57312\n",
            "Epoch 47/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 18.4176 - val_loss: 39.0465\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 30.57312\n",
            "Epoch 48/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 18.0975 - val_loss: 39.2126\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 30.57312\n",
            "Epoch 49/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 17.7498 - val_loss: 39.4852\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 30.57312\n",
            "Epoch 50/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 17.5431 - val_loss: 39.6628\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 30.57312\n",
            "Epoch 51/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 17.2594 - val_loss: 39.7685\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 30.57312\n",
            "Epoch 52/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 16.7910 - val_loss: 39.8738\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 30.57312\n",
            "Epoch 53/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 16.6893 - val_loss: 39.9564\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 30.57312\n",
            "Epoch 54/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 16.1274 - val_loss: 40.2576\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 30.57312\n",
            "Epoch 55/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 15.8733 - val_loss: 40.4475\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 30.57312\n",
            "Epoch 56/128\n",
            "1/1 [==============================] - 61s 61s/step - loss: 15.7989 - val_loss: 40.6870\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 30.57312\n",
            "Epoch 57/128\n",
            "1/1 [==============================] - 61s 61s/step - loss: 15.5619 - val_loss: 41.0616\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 30.57312\n",
            "Epoch 58/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 15.5364 - val_loss: 41.3082\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 30.57312\n",
            "Epoch 59/128\n",
            "1/1 [==============================] - 61s 61s/step - loss: 14.8614 - val_loss: 41.4324\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 30.57312\n",
            "Epoch 60/128\n",
            "1/1 [==============================] - 61s 61s/step - loss: 14.7665 - val_loss: 41.6221\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 30.57312\n",
            "Epoch 61/128\n",
            "1/1 [==============================] - 61s 61s/step - loss: 14.3794 - val_loss: 41.9008\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 30.57312\n",
            "Epoch 62/128\n",
            "1/1 [==============================] - 61s 61s/step - loss: 14.2950 - val_loss: 42.4009\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 30.57312\n",
            "Epoch 63/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 13.9789 - val_loss: 42.4789\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 30.57312\n",
            "Epoch 64/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 13.8714 - val_loss: 42.3658\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 30.57312\n",
            "Epoch 65/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 13.4785 - val_loss: 42.5554\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 30.57312\n",
            "Epoch 66/128\n",
            "1/1 [==============================] - 59s 59s/step - loss: 13.3063 - val_loss: 42.8997\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 30.57312\n",
            "Epoch 67/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 13.0415 - val_loss: 43.1129\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 30.57312\n",
            "Epoch 68/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 13.0146 - val_loss: 43.5477\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 30.57312\n",
            "Epoch 69/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 12.8470 - val_loss: 43.8613\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 30.57312\n",
            "Epoch 70/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 12.9130 - val_loss: 43.8533\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 30.57312\n",
            "Epoch 71/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 12.4265 - val_loss: 43.7402\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 30.57312\n",
            "Epoch 72/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 12.3260 - val_loss: 43.7645\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 30.57312\n",
            "Epoch 73/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 12.2304 - val_loss: 44.0709\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 30.57312\n",
            "Epoch 74/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 12.0910 - val_loss: 44.5048\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 30.57312\n",
            "Epoch 75/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 11.8592 - val_loss: 44.9568\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 30.57312\n",
            "Epoch 76/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 11.6225 - val_loss: 45.2733\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 30.57312\n",
            "Epoch 77/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 11.4320 - val_loss: 45.8253\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 30.57312\n",
            "Epoch 78/128\n",
            "1/1 [==============================] - 59s 59s/step - loss: 11.4256 - val_loss: 46.2315\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 30.57312\n",
            "Epoch 79/128\n",
            "1/1 [==============================] - 59s 59s/step - loss: 11.4736 - val_loss: 46.0687\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 30.57312\n",
            "Epoch 80/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 11.1473 - val_loss: 45.8521\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 30.57312\n",
            "Epoch 81/128\n",
            "1/1 [==============================] - 59s 59s/step - loss: 10.8915 - val_loss: 45.9137\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 30.57312\n",
            "Epoch 82/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 10.7004 - val_loss: 45.8939\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 30.57312\n",
            "Epoch 83/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 10.5590 - val_loss: 45.9164\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 30.57312\n",
            "Epoch 84/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 10.4224 - val_loss: 46.0424\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 30.57312\n",
            "Epoch 85/128\n",
            "1/1 [==============================] - 59s 59s/step - loss: 10.5812 - val_loss: 46.1730\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 30.57312\n",
            "Epoch 86/128\n",
            "1/1 [==============================] - 59s 59s/step - loss: 10.1875 - val_loss: 46.3589\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 30.57312\n",
            "Epoch 87/128\n",
            "1/1 [==============================] - 59s 59s/step - loss: 10.2288 - val_loss: 46.6770\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 30.57312\n",
            "Epoch 88/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 10.0206 - val_loss: 46.8682\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 30.57312\n",
            "Epoch 89/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 9.9909 - val_loss: 47.0432\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 30.57312\n",
            "Epoch 90/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 9.7161 - val_loss: 47.0119\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 30.57312\n",
            "Epoch 91/128\n",
            "1/1 [==============================] - 62s 62s/step - loss: 9.5714 - val_loss: 47.0435\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 30.57312\n",
            "Epoch 92/128\n",
            "1/1 [==============================] - 61s 61s/step - loss: 9.3723 - val_loss: 47.2667\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 30.57312\n",
            "Epoch 93/128\n",
            "1/1 [==============================] - 61s 61s/step - loss: 9.2845 - val_loss: 47.5329\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 30.57312\n",
            "Epoch 94/128\n",
            "1/1 [==============================] - 61s 61s/step - loss: 9.1774 - val_loss: 47.6133\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 30.57312\n",
            "Epoch 95/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 8.9919 - val_loss: 47.6488\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 30.57312\n",
            "Epoch 96/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 8.9500 - val_loss: 47.7305\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 30.57312\n",
            "Epoch 97/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 8.6811 - val_loss: 47.9877\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 30.57312\n",
            "Epoch 98/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 8.6935 - val_loss: 48.2034\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 30.57312\n",
            "Epoch 99/128\n",
            "1/1 [==============================] - 59s 59s/step - loss: 8.4145 - val_loss: 48.3963\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 30.57312\n",
            "Epoch 100/128\n",
            "1/1 [==============================] - 59s 59s/step - loss: 8.3986 - val_loss: 48.4884\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 30.57312\n",
            "Epoch 101/128\n",
            "1/1 [==============================] - 61s 61s/step - loss: 8.2910 - val_loss: 48.7560\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 30.57312\n",
            "Epoch 102/128\n",
            "1/1 [==============================] - 61s 61s/step - loss: 8.0623 - val_loss: 48.9838\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 30.57312\n",
            "Epoch 103/128\n",
            "1/1 [==============================] - 61s 61s/step - loss: 8.0570 - val_loss: 49.0092\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 30.57312\n",
            "Epoch 104/128\n",
            "1/1 [==============================] - 61s 61s/step - loss: 7.8847 - val_loss: 49.0642\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 30.57312\n",
            "Epoch 105/128\n",
            "1/1 [==============================] - 61s 61s/step - loss: 7.6293 - val_loss: 49.2351\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 30.57312\n",
            "Epoch 106/128\n",
            "1/1 [==============================] - 61s 61s/step - loss: 7.5544 - val_loss: 49.6773\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 30.57312\n",
            "Epoch 107/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 7.4740 - val_loss: 50.0055\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 30.57312\n",
            "Epoch 108/128\n",
            "1/1 [==============================] - 61s 61s/step - loss: 7.4016 - val_loss: 49.9618\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 30.57312\n",
            "Epoch 109/128\n",
            "1/1 [==============================] - 61s 61s/step - loss: 7.1210 - val_loss: 49.9388\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 30.57312\n",
            "Epoch 110/128\n",
            "1/1 [==============================] - 61s 61s/step - loss: 7.0944 - val_loss: 49.8334\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 30.57312\n",
            "Epoch 111/128\n",
            "1/1 [==============================] - 61s 61s/step - loss: 7.0520 - val_loss: 50.1505\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 30.57312\n",
            "Epoch 112/128\n",
            "1/1 [==============================] - 61s 61s/step - loss: 6.8861 - val_loss: 50.1598\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 30.57312\n",
            "Epoch 113/128\n",
            "1/1 [==============================] - 61s 61s/step - loss: 6.8772 - val_loss: 50.7018\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 30.57312\n",
            "Epoch 114/128\n",
            "1/1 [==============================] - 61s 61s/step - loss: 6.8647 - val_loss: 51.4064\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 30.57312\n",
            "Epoch 115/128\n",
            "1/1 [==============================] - 61s 61s/step - loss: 6.7131 - val_loss: 51.6915\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 30.57312\n",
            "Epoch 116/128\n",
            "1/1 [==============================] - 61s 61s/step - loss: 6.5883 - val_loss: 51.9269\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 30.57312\n",
            "Epoch 117/128\n",
            "1/1 [==============================] - 61s 61s/step - loss: 6.3957 - val_loss: 52.4309\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 30.57312\n",
            "Epoch 118/128\n",
            "1/1 [==============================] - 61s 61s/step - loss: 6.4244 - val_loss: 53.3587\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 30.57312\n",
            "Epoch 119/128\n",
            "1/1 [==============================] - 61s 61s/step - loss: 6.1821 - val_loss: 54.5403\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 30.57312\n",
            "Epoch 120/128\n",
            "1/1 [==============================] - 61s 61s/step - loss: 6.0792 - val_loss: 55.4063\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 30.57312\n",
            "Epoch 121/128\n",
            "1/1 [==============================] - 61s 61s/step - loss: 6.0461 - val_loss: 55.9884\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 30.57312\n",
            "Epoch 122/128\n",
            "1/1 [==============================] - 61s 61s/step - loss: 6.0035 - val_loss: 56.1418\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 30.57312\n",
            "Epoch 123/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 5.9178 - val_loss: 56.7484\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 30.57312\n",
            "Epoch 124/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 5.8785 - val_loss: 57.2780\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 30.57312\n",
            "Epoch 125/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 5.8162 - val_loss: 56.6186\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 30.57312\n",
            "Epoch 126/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 5.9011 - val_loss: 55.6417\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 30.57312\n",
            "Epoch 127/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 5.5352 - val_loss: 55.2196\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 30.57312\n",
            "Epoch 128/128\n",
            "1/1 [==============================] - 60s 60s/step - loss: 5.4928 - val_loss: 55.4813\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 30.57312\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "FsrzL4awHocV",
        "outputId": "1212ed0a-4d7e-463c-81fa-ff658f9df598"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# summarize history for loss\n",
        "plt.plot(hist.history['loss'])\n",
        "plt.plot(hist.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "od6JwVUvId7Z",
        "outputId": "bf5afa49-bcbc-46dd-9d0b-cdf5c820128d"
      },
      "source": [
        "# load the saved best model weights\n",
        "act_model.load_weights('/content/CRNN_model.hdf5')\n",
        " \n",
        "# predict outputs on validation images\n",
        "prediction = act_model.predict(valid_img[:10])\n",
        " \n",
        "# use CTC decoder\n",
        "out = K.get_value(K.ctc_decode(prediction, input_length=np.ones(prediction.shape[0])*prediction.shape[1],\n",
        "                         greedy=True)[0][0])\n",
        " \n",
        "# see the results\n",
        "i = 0\n",
        "for x in out:\n",
        "    print(\"original_text =  \", valid_orig_txt[i])\n",
        "    print(\"predicted text = \", end = '')\n",
        "    for p in x:  \n",
        "        if int(p) != -1:\n",
        "            print(char_list[int(p)], end = '')       \n",
        "    print('\\n')\n",
        "    i+=1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original_text =   APOPLEXIES\n",
            "predicted text = \n",
            "\n",
            "original_text =   overran\n",
            "predicted text = \n",
            "\n",
            "original_text =   wesleyan\n",
            "predicted text = \n",
            "\n",
            "original_text =   admirals\n",
            "predicted text = \n",
            "\n",
            "original_text =   Pledge\n",
            "predicted text = \n",
            "\n",
            "original_text =   Agate\n",
            "predicted text = \n",
            "\n",
            "original_text =   Catalyst\n",
            "predicted text = \n",
            "\n",
            "original_text =   MOUE\n",
            "predicted text = \n",
            "\n",
            "original_text =   Contemplating\n",
            "predicted text = \n",
            "\n",
            "original_text =   TALLYHOED\n",
            "predicted text = \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}